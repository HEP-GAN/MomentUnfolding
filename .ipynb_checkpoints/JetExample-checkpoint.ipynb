{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f5ae9-1628-4ccc-9bf9-8228b6158dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Layer, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "plt.rc('font', size=20)\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede61abc-29b5-44ff-9539-b53da0f9453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = 'q' #Choose the observable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1de8e2-0c10-4769-99de-b6466f2779bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load and normalize the data\n",
    "data = np.load('npfiles/rawdata.npz')\n",
    "substructure_variables = ['pT', 'w', 'q', 'm', 'r', 'tau1s', 'tau2s']\n",
    "data_streams = ['_true', '_true_alt', '_reco', '_reco_alt']\n",
    "n_variables = len(substructure_variables)\n",
    "\n",
    "\n",
    "normalize = True\n",
    "    \n",
    "for stream in data_streams:\n",
    "    globals()['x'+stream] = data[obs+stream][:150000]\n",
    "    \n",
    "xm, xs = (x_true_alt.mean(), x_true_alt.std()) if normalize else (0, 1)\n",
    "for stream in data_streams:\n",
    "    globals()['x'+stream] = (globals()['x'+stream] - xm)/xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd29cc-5552-42a3-ba8b-536a7c3cbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(Layer):\n",
    "    def __init__(self, myc, **kwargs):\n",
    "        self.myinit = myc\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self._lambda0 = self.add_weight(name='lambda0', \n",
    "                                    shape=(1,),\n",
    "                                    initializer=tf.keras.initializers.Constant(self.myinit), \n",
    "                                    trainable=True)\n",
    "        self._lambda1 = self.add_weight(name='lambda1', \n",
    "                                    shape=(1,),\n",
    "                                    initializer=tf.keras.initializers.Constant(self.myinit), \n",
    "                                    trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.exp(self._lambda0 * x + self._lambda1 * x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e66565-e205-4dba-8ade-345361c4c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mlc(y_true, y_pred):\n",
    "    weights = tf.gather(y_true, [1], axis=1) # event weights\n",
    "    y_true = tf.gather(y_true, [0], axis=1) # actual y_true for loss\n",
    "    \n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = -weights * ((y_true) * K.log(y_pred) +\n",
    "                         (1 - y_true) * (1 - y_pred))\n",
    "    return K.mean(t_loss)\n",
    "\n",
    "def weighted_mlc_GAN(y_true, y_pred):\n",
    "    weights = tf.gather(y_pred, [1], axis=1) # event weights\n",
    "    y_pred = tf.gather(y_pred, [0], axis=1) # actual y_pred for loss\n",
    "    \n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "    \n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = weights * ((1 - y_true) * (1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68b7cf-6ff6-4310-8870-b3f045dc3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals_1 = np.concatenate([x_true_alt,x_true])\n",
    "xvals_2 = np.concatenate([x_reco_alt,x_reco])\n",
    "yvals = np.concatenate([np.ones(len(x_true_alt)),np.zeros(len(x_true))])\n",
    "\n",
    "X_train_1, X_test_1, X_train_2, X_test_2, Y_train_1, Y_test_1 = train_test_split(xvals_1, xvals_2, yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dae976-6e77-408a-9d3e-0d6124eadaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "myc = np.random.normal()\n",
    "\n",
    "mymodel_inputtest = Input(shape=(1,))\n",
    "mymodel_test = MyLayer(myc)(mymodel_inputtest)\n",
    "model_generator = Model(mymodel_inputtest, mymodel_test)\n",
    "\n",
    "inputs_disc = Input((1, ))\n",
    "hidden_layer_1_disc = Dense(50, activation='relu')(inputs_disc)\n",
    "hidden_layer_2_disc = Dense(50, activation='relu')(hidden_layer_1_disc)\n",
    "hidden_layer_3_disc = Dense(50, activation='relu')(hidden_layer_2_disc)\n",
    "outputs_disc = Dense(1, activation='sigmoid')(hidden_layer_3_disc)\n",
    "model_discrimantor = Model(inputs=inputs_disc, outputs=outputs_disc)\n",
    "\n",
    "model_discrimantor.compile(loss=weighted_mlc, optimizer='adam')\n",
    "\n",
    "model_discrimantor.trainable = False\n",
    "mymodel_gan = Input(shape=(1,))\n",
    "gan_model = Model(inputs=mymodel_gan,outputs=concatenate([model_discrimantor(mymodel_gan),model_generator(mymodel_gan)]))\n",
    "\n",
    "gan_model.compile(loss=weighted_mlc_GAN, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568761e-94b4-495b-8922-a2fa4b070600",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "n_batch = 128*10\n",
    "n_batches = len(X_train_1) // n_batch\n",
    "lambdas = []\n",
    "\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    for j in range(n_batches):\n",
    "        X_batch_1 = X_train_1[j*n_batch:(j+1)*n_batch]\n",
    "        X_batch_2 = X_train_1[j*n_batch:(j+1)*n_batch]\n",
    "        Y_batch = Y_train_1[j*n_batch:(j+1)*n_batch]\n",
    "        W_batch = model_generator(X_batch_1)\n",
    "        W_batch = np.array(W_batch).flatten()\n",
    "        W_batch[Y_batch==1] = 1        \n",
    "        Y_batch_2 = np.stack((Y_batch, W_batch), axis=1)\n",
    "        \n",
    "        model_discrimantor.train_on_batch(X_batch_2, Y_batch_2)        \n",
    "        gan_model.train_on_batch(X_batch_1[Y_batch==0],np.zeros(len(X_batch_2[Y_batch==0])))\n",
    "    lambdasum = np.log(model_generator.predict([1.], verbose = 0))\n",
    "    lambdasum2 = np.log(model_generator.predict([2.], verbose = 0))\n",
    "    mylambda1 = (lambdasum2-2*lambdasum)/2\n",
    "    mylambda0 = lambdasum - mylambda1\n",
    "    print(\"on epoch=\",i,mylambda0,mylambda1)\n",
    "    lambdas += [mylambda0, mylambda1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda3314-4135-45d8-b269-04fd9ba3ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = X_test_1[Y_test_1==0] \n",
    "gen = X_test_1[Y_test_1==1]\n",
    "data = X_test_2[Y_test_1==0]\n",
    "sim = X_test_2[Y_test_1==1]\n",
    "\n",
    "total_err = []\n",
    "\n",
    "for lambda0, lambda1 in lambdas:\n",
    "    weights = np.exp(lambda1*gen**2+lambda0*gen)*len(data)/np.sum(np.exp(lambda1*gen**2+lambda0*gen))\n",
    "    mean_err = (np.average(gen, weights = weights - truth.mean())\n",
    "    var_err = np.average(gen**2, weights = weights - np.mean(truth**2)\n",
    "    total_err += [(mean_err)**2 + var_err]\n",
    "    \n",
    "lambda0, lambda1 = lambdas[np.argmin(total_err)]\n",
    "weights = np.exp(lambda1*gen**2+lambda0*gen)*len(data)/np.sum(np.exp(lambda1*gen**2+lambda0*gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2dc20-374d-46f3-b297-7e65dde4acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = X_test_1[Y_test_1==0] * xs + xm\n",
    "gen = X_test_1[Y_test_1==1] * xs + xm\n",
    "data = X_test_2[Y_test_1==0] * xs + xm\n",
    "sim = X_test_2[Y_test_1==1] * xs + xm\n",
    "\n",
    "bins = np.linspace(truth.min(), truth.max(), 30)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "_,_,_=plt.hist(truth, bins=bins, alpha=0.5, label=\"Truth\", density=True)\n",
    "_,_,_=plt.hist(gen, bins=bins, alpha=0.5, label=\"Generation\", density=True)\n",
    "_,_,_=plt.hist(gen, bins=bins, weights=weights, histtype=\"step\", color='black', ls=\":\", lw=4, label=\"Moment Unfolding\", density=True)\n",
    "\n",
    "plt.legend(fontsize=24)\n",
    "plt.xlabel(\"z (particle level)\", fontsize=24)\n",
    "plt.ylabel(\"Counts\", fontsize=24)\n",
    "plt.title(f\"Jet {obs} Data: Particle Level Histograms\", fontsize=24)\n",
    "\n",
    "plt.savefig(f\"figures/{obs}jetexample.pdf\", bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39f572-a05c-4c1f-bac6-914778223b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65b80d-f82b-4c80-ad35-f609d81254ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.0",
   "language": "python",
   "name": "tensorflow-2.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
